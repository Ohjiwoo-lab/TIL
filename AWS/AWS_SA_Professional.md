
-----------------------

### #1

솔루션 설계자는 회사에서 곧 출시할 새 애플리케이션을 위한 데이터 저장 및 검색 아키텍처를 설계하고 있습니다. 이 애플리케이션은 전 세계 장치에서 분당 수백만 개의 작은 레코드를 수집하도록 설계되었습니다. 각 레코드의 크기는 4KB 미만이며 짧은 대기 시간으로 검색할 수 있는 내구성 있는 위치에 저장되어야 합니다. 데이터는 일시적이며 회사는 120일 동안만 데이터를 저장해야 하며 그 이후에는 데이터가 삭제될 수 있습니다.

솔루션 설계자는 1년 동안 스토리지 요구 사항이 약 10~15TB가 될 것으로 계산합니다.

가장 비용 효율적이고 설계 요구 사항을 충족하는 스토리지 전략은 무엇입니까?

- A. 인덱싱된 검색을 허용하기 위해 각 수신 레코드를 Amazon S3 버킷에 단일 .csv 파일로 저장하도록 애플리케이션을 설계합니다. 120일이 지난 데이터를 삭제하도록 수명 주기 정책을 구성합니다.

- B. 규모에 맞게 적절하게 구성된 Amazon DynamoDB 테이블에 각 수신 레코드를 저장하도록 애플리케이션을 설계합니다. 120일보다 오래된 레코드를 삭제하도록 DynamoDB TTL(Time to Live) 기능을 구성합니다.

- C. 각 수신 레코드를 Amazon RDS MySQL 데이터베이스의 단일 테이블에 저장하도록 애플리케이션을 설계합니다. 120일보다 오래된 레코드를 삭제하는 쿼리를 실행하는 야간 크론 작업을 실행합니다.

- D. 수신 레코드를 Amazon S3 버킷에 쓰기 전에 일괄 처리하도록 애플리케이션을 설계합니다. 배치의 레코드 목록을 포함하도록 객체의 메타데이터를 업데이트하고 Amazon S3 메타데이터 검색 기능을 사용하여 데이터를 검색합니다. 120일 후에 데이터를 삭제하도록 수명 주기 정책을 구성합니다.

<details>
   <summary> <b> 문제 풀이 확인 (👈click!) </b> </summary>

<br/>

이는 데이터를 저장할 적절한 스토리지를 선택하는 문제이다. 요구 사항을 더 살펴보면 저장할 데이터는 `분당 수백만 개`가 생성되는 `크기가 작은` 레코드이며, 짧은 대기 시간으로 `검색`할 수 있어야 하고, `내구성`이 있는 위치여야 하며, 120일이 지난 후 `삭제`되어야 한다. 또한 1년 동안 `10~15TB` 정도의 꽤 많은 양의 데이터를 저장해야 하고, `비용`이 가장 적게 들어야 한다.

보기를 보면 스토리지 후보로 `S3`, `DynamoDB`, `RDS MySQL`이 제시되었다. S3부터 살펴보자.

S3는 무한히 확장할 수 있기 때문에 10~15TB의 저장 공간 요구사항을 충족하고, 99.999999999%의 데이터 내구성을 가지고 있어 저장된 데이터가 손실될 걱정은 할 필요가 없다. 또한 수명주기 정책을 통해 120일이 지난 후 자동으로 데이터가 삭제되도록 할 수 있으며, 비용 효율적인 스토리지이다.

하지만 작은 파일을 많이 저장하거나, 검색 기능을 충족하지는 못한다. S3에 크기가 작은 파일을 많이 저장하는 것이 권장되지 않는 이유는 S3 객체에 대한 GET 요청 제한 때문이다. S3에서는 접두상 1개 당 초 당 5500개의 GET 요청을 할 수 있다. 만약 S3에 크기가 작은 파일들을 수백 만개 저장해두고 있었다면, 서비스를 제공하기 위해 빈번한 GET 요청이 이루어질 것이고 이로 인해 제한을 받을 수 있기 때문에 되도록 크기가 작은 파일을 많이 저장하는 경우 S3는 좋지 못한 선택지이다.

또한 S3는 속성 기반 검색을 제공해주지 않는다. S3 Select를 통해 S3에 저장된 데이터의 일부분을 필터링하여 결과를 반환할 수 있는 기능이 존재하긴 하지만, 데이터 형식이 JSON, CSV 등으로 제한되어 있고 하나의 데이터를 검색하기 위해 일일히 SQL 쿼리문을 작성해야 한다는 불편함이 있다. 그래서 S3에서 데이터를 검색하기 위해서는 DynamoDB 테이블에 객체에 대한 메타데이터를 저장하는 방식이 있다. 아키텍처는 다음과 같을 것이다.

![image](https://github.com/Ohjiwoo-lab/TIL/assets/74577768/271d6136-e0a5-43f4-9f1e-75b8921327c6)   
출처: [Building and Maintaining an Amazon S3 Metadata Index without Servers](https://aws.amazon.com/ko/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers/)

S3에 데이터가 업로드되면 S3 Events를 통해 Lambda 함수를 트리거한다. 그러면 Lambda 함수는 업로드된 객체의 메타데이터를 추출하여 검색 가능한 인덱스를 DynamoDB 테이블에 저장한다. 그러면 추후 DynamoDB 테이블을 통해 객체를 검색할 수 있게 된다.

A에서는 인덱싱된 검색을 위해 CSV 파일에 모든 데이터를 저장하도록 설계한다고 했는데, 이는 S3 Select를 이용하기 위함이다. 이는 분명 가능한 선택지이기는 하지만 분당 수백만 개씩 생성되는 데이터를 CSV 파일에 저장하기 위해 일일히 변환 작업을 수행해야 하고, 데이터를 검색할 때에도 SQL 쿼리문을 작성해야 한다는 점에서 시간이 오래 걸리고 많은 비용이 발생할 것이다. 따라서 적합하지 않다.

D에서는 메타 데이터 검색 기능을 사용한다고 되어 있는데, 이는 S3에서 제공해주지 않는 기능이기 때문에 옳은 선택지가 아니다.

다음으로 RDS MySQL은 범용 SSD를 사용하는 경우 20GiB~64TiB의 용량을 제공해줄 수 있기 때문에 저장 공간 요구사항을 충족하고, 높은 내구성과 검색 기능도 충족한다. 하지만 RDS는 비용이 상당히 비싼 서비스이며 데이터를 자동으로 삭제해주는 기능을 제공해주지 못한다.

C에서 야간 크론 작업을 통해 데이터를 120일 후에 자동으로 삭제하도록 구성한다고 했는데, 물론 가능한 작업이지만 매번 크론 작업을 실행하는 데에 비용이 추가로 발생할 것이며 운영하기에도 복잡할 것이다. C는 비용 측면이나 운영 측면에서도 모두 효율적이지 못한 선택지이다.

따라서 정답은 B가 된다. DynamoDB는 분당 수백만 개씩 생성되는 데이터에 맞게 빠르게 확장할 수 있고, AWS에 의해 기본적으로 내구성이 보장되며, 모든 규모에서 10밀리초 미만의 짧은 대기 시간을 제공한다. 사용자가 정의한 프라이머리 키를 이용한 GET 작업과, 글로벌 보조 인덱스와 로컬 보조 인덱스를 통해 키가 아닌 속성 기반으로 검색할 수 있고, TTL 기능을 통해 일정 시간이 지난 후 데이터가 삭제되도록 설정할 수 있다. DynamoDB는 S3와 다르게 항목의 최대 크기가 400KB를 초과할 수 없다. 즉, 크기가 작은 데이터를 많이 저장하기에 적합한 스토리지인 것이다. 또한 서버리스 서비스이기 때문에 사용한 만큼만 비용을 지불하면 되어 비용 측면에서도 효율적이다.

따라서 DynamoDB가 해당 문제의 가장 적합한 스토리지이다.


</details>

-----------------------

<br />

### #2

한 회사가 AWS에서 SaaS(Software-as-a-Service) 솔루션을 구축하고 있습니다. 이 회사는 여러 AWS 지역과 동일한 프로덕션 계정에 AWS Lambda 통합을 통해 Amazon API Gateway REST API를 배포했습니다.

이 회사는 고객이 초당 특정 수의 API 호출을 수행할 수 있는 용량에 대해 비용을 지불할 수 있는 계층화된 가격을 제공합니다. 프리미엄 계층은 초당 최대 3,000개의 호출을 제공하며 고객은 고유한 API 키로 식별됩니다. 다양한 지역의 몇몇 프리미엄 등급 고객은 사용량이 가장 많은 시간 동안 여러 API 메서드로부터 429 요청이 너무 많다는 오류 응답을 받았다고 보고합니다. 로그에는 Lambda 함수가 호출되지 않았음을 나타냅니다.

이러한 고객에게 나타나는 오류 메시지의 원인은 무엇입니까?

- A. Lambda 함수가 동시성 제한에 도달했습니다.
- B. Lambda 함수의 동시성 지역 제한.
- C. 회사가 API Gateway 계정의 초당 호출 한도에 도달했습니다.
- D. 회사는 초당 호출에 대한 API 게이트웨이 기본 메서드별 제한에 도달했습니다.

<details>
   <summary> <b> 문제 풀이 확인 (👈click!) </b> </summary>

<br/>

현재 회사는 SaaS 솔루션을 API Gateway와 Lambda 함수를 이용하여 구축하고 있다. 그런데 문제는 사용량이 가장 많은 시간에 `429` 오류가 발생한다는 것이다. 로그에는 Lambda 함수가 호출되지 않았음을 나타냈기 때문에 이 오류는 API Gateway에서 발생한 오류일 것이다.

다음은 API Gateway에서 흔히 발생하는 오류 유형이다.

**4xx**: 클라이언트 측에서 발생된 오류이다.

- 400: Bad Request. 유효하지 않은 요청 메시지가 포함되어 있는 등 잘못된 요청을 받아 처리할 수 없는 경우이다.

- 403: Access Denied, WAF Filtered. 요청을 보낸 클라이언트에 적절한 권한이 없어서 거부된 경우이다.

- 429: Quota exceeded, Throttle. 할당량을 초과하는 과도한 요청이 들어온 경우 발생한다.

**5xx**: 서버 측에서 발생된 오류이다.

- 502: Bad Gateway. 백단으로부터 유효하지 않거나 잘못된 응답을 받은 경우이다.

- 503: Service Unavailable Exception. 백단의 서버가 성능 문제나 유지보수 등으로 인해 응답을 할 수 없는 경우 일시적으로 서비스 사용이 불가능함을 의미한다.

- 504: Integration Failure. API Gateway의 요청 타임아웃인 29초를 초과한 경우에 발생한다.

현재 백단인 Lambda 함수는 호출되지도 않았으므로 5xx 에러가 아닌 클라이언트 측 에러임을 알 수 있다. 특히 사용량이 급증했을 때 발생했다고 했으므로 문제에서 따로 언급하지 않더라도 429 에러임을 짐작할 수 있다. 따라서 사전에 할당되어 있었던 API Gateway의 호출 한도가 초과했기 때문에 오류가 발생했으므로 C가 정답이다.

</details>

-----------------------

<br />